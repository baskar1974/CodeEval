{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ad02fd-9901-494d-b022-7a4f31f92622",
   "metadata": {},
   "source": [
    "## Logging into my HuggingFace account for using StarCoder, as it is a gated rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4cbc0-68a9-4421-913e-7fc54669759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35790d-9bb6-4c36-ba4c-b5c6a83d9745",
   "metadata": {},
   "source": [
    "## Reading the test data which is being for evaluation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bce3e-71a2-496a-8baa-bb5e40c35b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the questions and corresponding code snippets\n",
    "data = pd.read_csv(\"/home/baskar/CALIX_LLM/CodeGenEvalPipeline/experiments/python_questions_answers.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc82a2-3a9d-4a5a-a8f6-d24a6f43d8ca",
   "metadata": {},
   "source": [
    "## Downloading the Tokenizer and the Generating model (here : StarCoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de207c-c332-491c-ba8c-252a8486401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "# Initialize Starcoder model\n",
    "\n",
    "model_name = 'bigcode/starcoder'\n",
    "\n",
    "#quantize the model\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bff25c-cb88-4ecc-b99f-b17112ef238e",
   "metadata": {},
   "source": [
    "## Generating Code from the DataFrame that being created from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1cdcc-f022-4143-b4b4-f292ac5f4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate code\n",
    "def generate_code_from_question(question: str, tokenizer, model) -> str:\n",
    "    inputs = tokenizer(question, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'].to(model.device),\n",
    "            max_length=150,  # Adjust based on your needs\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply code generation\n",
    "data['generated_code'] = df['question'].apply(lambda q: generate_code_from_question(q, tokenizer, model))\n",
    "\n",
    "# Display the DataFrame with generated code\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145711e-7b39-4d70-be3d-88979b10a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"generated_code\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69efdf-4536-4970-892a-eac1ec4f91c2",
   "metadata": {},
   "source": [
    "## Evaluating Cyclomatic Complexity\n",
    "\n",
    "Cyclomatic complexity is calculated based on the control flow graph of a program. The graph consists of nodes (representing blocks of code) and edges (representing control flow between these blocks).\n",
    "\n",
    "The cyclomatic complexity V(G)=Eâˆ’N+ 2P:\n",
    "        # Where E - Number of edges in the control flow graph, N - Number of nodes in the control flow graph, P - Number of connected components in the            graph (usually 1 for a single program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012646d7-e8ef-450d-af2f-30a2c7d50f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radon.complexity import cc_visit, cc_rank\n",
    "def cyclomatic_complexity(generated_code):\n",
    "    # Define the code you want to analyze\n",
    "    code = generated_code\n",
    "    \n",
    "    # Compute the cyclomatic complexity\n",
    "    complexity_info = cc_visit(code)\n",
    "    \n",
    "    # Display the results\n",
    "    for item in complexity_info:\n",
    "        print(f\"Function Name: {item.name}\")\n",
    "        print(f\"Cyclomatic Complexity: {item.complexity}\")\n",
    "        print(f\"Complexity Rank: {cc_rank(item.complexity)}\")\n",
    "df['generated_code'].apply(lambda q: cyclomatic_complexity(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ff656-febe-4486-ae2c-f4b4523b989c",
   "metadata": {},
   "source": [
    "## ROUGE score\n",
    "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score is a set of metrics used to evaluate the quality of summaries or text generated by comparing them to reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab519e-b714-4848-9463-0e9f80f2b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example ground truth and generated code\n",
    "ground_truth_code = ground_truth\n",
    "\n",
    "# Create a ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(ground_truth_code, generated_code)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "for key in scores:\n",
    "    print(f\"{key}: Precision: {scores[key].precision:.4f}, Recall: {scores[key].recall:.4f}, F1-Score: {scores[key].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef3aa3-bde2-45ae-b32d-1808943a3775",
   "metadata": {},
   "source": [
    "## Linting\n",
    "Linting is the process of analyzing source code to identify and report potential errors, stylistic issues, and other inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620e10f-c204-4b51-81f7-4d3792b60a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylint\n",
    "import subprocess\n",
    "\n",
    "code = generated_code\n",
    "\n",
    "with open('lint_test.py', 'w') as f:\n",
    "    f.write(code)\n",
    "\n",
    "def lint_with_pylint(file_path):\n",
    "    \"\"\"Run pylint on the specified file and return the output.\"\"\"\n",
    "    result = subprocess.run(['pylint', file_path], capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "# Check the code and print results\n",
    "pylint_output = lint_with_pylint('example.py')\n",
    "print(\"pylint Linting Results:\")\n",
    "print(pylint_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec36c9e-4d5d-4133-9377-7532463af9c0",
   "metadata": {},
   "source": [
    "## Embeddings Similarity \n",
    "Embedding similarity is a technique used to measure the similarity between data points (e.g., words, sentences, or documents) by comparing their vector representations. Embeddings are dense vector representations of data points that capture semantic meaning, and similarity measures help determine how closely related or similar two embeddings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832c6a5-2f8c-430e-a8d8-dffe113c88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained model for code embeddings\n",
    "model = SentenceTransformer('microsoft/codebert-base')\n",
    "\n",
    "# Example generated code and ground truth code\n",
    "generated_code = generated_code\n",
    "ground_truth_code = ground_truth\n",
    "\n",
    "# Compute embeddings\n",
    "generated_embedding = model.encode(generated_code)\n",
    "ground_truth_embedding = model.encode(ground_truth_code)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = util.cos_sim(generated_embedding, ground_truth_embedding)\n",
    "\n",
    "print(f\"Code Similarity: {similarity.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
